{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T18:57:36.476095Z",
     "start_time": "2020-11-05T18:57:31.741478Z"
    },
    "id": "PzRKgXa8Fwc0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import code\n",
    "import urllib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T18:57:36.562269Z",
     "start_time": "2020-11-05T18:57:36.478886Z"
    },
    "id": "1XCqej8mFwc1"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T18:57:36.693949Z",
     "start_time": "2020-11-05T18:57:36.566770Z"
    },
    "id": "uMaqi7NPFwc1"
   },
   "outputs": [],
   "source": [
    "sys.path.append('./')\n",
    "from helpers.helpers import *\n",
    "from helpers.mask_to_submission import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ohxvz99vXOkY"
   },
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "RESTORE_MODEL = False  # If True, restore existing model instead of training a new one\n",
    "GENERATE_PREDICTION = False #If True, will generate a CSV to submit on AICrowd\n",
    "\n",
    "PREDICTIONS_SAVE_DIR = 'predictions/'\n",
    "MODELS_SAVE_DIR = 'model_save/'\n",
    "MODEL_NAME = 'u-net' #Chose between cnn_handmade and u-net\n",
    "\n",
    "TRAINING_SAMPLES = 100 #max 100\n",
    "\n",
    "TRAINING_SIZE = 80 # Size of the training set in percentage, integer between 0 and 100, the remaining part is for testing\n",
    "VALIDATION_SIZE = 0.20  # Size of the validation set, float between 0 and 1\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_CHANNELS = 3  # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "BATCH_SIZE = 16  # 64\n",
    "RECORDING_STEP = 0\n",
    "\n",
    "# Set image patch size in pixels\n",
    "# IMG_PATCH_SIZE should be a multiple of 4\n",
    "# image size should be an integer multiple of this number!\n",
    "IMG_PATCH_SIZE = 16\n",
    "IMAGE_SIZE = 400\n",
    "if IMAGE_SIZE % IMG_PATCH_SIZE != 0 :\n",
    "    print('Warning : the image size is not a multiple of the patch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBdjGm_UPHP1",
    "outputId": "b6659e33-ec54-44f4-d404-d7c36f90d251"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    #Set current directory and clone github repo\n",
    "    from getpass import getpass\n",
    "\n",
    "    drive_path = '/content/drive/MyDrive/Colab_data/'\n",
    "    os.chdir(drive_path)\n",
    "    repo = \"CS-433-project-2\"\n",
    "\n",
    "    if not os.path.isdir(\"CS-433-project-2\"):\n",
    "        uname = input(\"Github username:\")    \n",
    "        password = getpass('Password:')\n",
    "        !git clone 'https://{uname}:{password}@github.com/Julien-Ben/{repo}.git'\n",
    "    else:\n",
    "        os.chdir(repo)\n",
    "        !git pull\n",
    "    %cd project_road_segmentation\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO move into a python file for Unet model\n",
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    p = layers.MaxPool2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    us = layers.UpSampling2D((2, 2))(x)\n",
    "    concat = layers.Concatenate()([us, skip])\n",
    "    c = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
    "    c = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO move into a python file for Unet model\n",
    "def UNet():\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f[0]) #128 -> 64\n",
    "    c2, p2 = down_block(p1, f[1]) #64 -> 32\n",
    "    c3, p3 = down_block(p2, f[2]) #32 -> 16\n",
    "    c4, p4 = down_block(p3, f[3]) #16->8\n",
    "    \n",
    "    bn = bottleneck(p4, f[4])\n",
    "    \n",
    "    u1 = up_block(bn, c4, f[3]) #8 -> 16\n",
    "    u2 = up_block(u1, c3, f[2]) #16 -> 32\n",
    "    u3 = up_block(u2, c2, f[1]) #32 -> 64\n",
    "    u4 = up_block(u3, c1, f[0]) #64 -> 128\n",
    "    \n",
    "    outputs = layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T19:25:32.315142Z",
     "start_time": "2020-11-05T19:25:31.979499Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQj_VzubFwc5",
    "outputId": "e8a15e1c-5611-459f-b1d3-0803cf43ac14",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CNN() : \n",
    "    # Create the model\n",
    "    cnn_model = models.Sequential()\n",
    "    cnn_model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=(400, 400, 3), padding='same', use_bias=True))\n",
    "    cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "    cnn_model.add(layers.Conv2D(128, (5, 5), activation='relu', input_shape=(8, 8, 3), padding='same', use_bias=True))\n",
    "    cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "    cnn_model.add(layers.Flatten())\n",
    "    cnn_model.add(layers.Dense(128, activation='relu'))\n",
    "    cnn_model.add(layers.Dense(2))\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'cnn_handmade' : {\n",
    "        'name' : 'cnn',\n",
    "        'model' : CNN,\n",
    "        'save_dir' : MODELS_SAVE_DIR + 'cnn_handmade/'\n",
    "    },\n",
    "    'u-net' : {\n",
    "        'name' : 'u-net',\n",
    "        'model' : UNet,\n",
    "        'save_dir' : MODELS_SAVE_DIR + 'cnn_handmade/'\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chosen = models_dict[MODEL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T19:24:31.064890Z",
     "start_time": "2020-11-05T19:24:24.322395Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpiPmLxiFwc1",
    "outputId": "82985872-cadf-4a67-cecd-05b6336210ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 108.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1223.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 training images\n",
      "Loaded 10 training images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    imgs = []\n",
    "    for i in tqdm(range(1, num_images+1)):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print('File ' + image_filename + ' does not exist')\n",
    "            \n",
    "    print('Loaded {} training images'.format(len(imgs)))\n",
    "    return np.asarray(imgs)\n",
    "\n",
    "images = extract_data(train_data_filename, TRAINING_SAMPLES)\n",
    "groundtruths = extract_data(train_labels_filename, TRAINING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400, 3)\n",
      "(400, 400, 3)\n",
      "(400, 400)\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape)\n",
    "images_mean = images#.mean(axis=3)\n",
    "print(images_mean[0].shape)\n",
    "print(groundtruths[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIHX7_a4XnNu",
    "outputId": "9e4e59d8-0d6a-446d-a108-60be01d9205a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T19:24:31.261976Z",
     "start_time": "2020-11-05T19:24:31.067813Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyMhDusfFwc3",
    "outputId": "f65702a4-b2f3-4b7e-d52f-5dcf3a0b770b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 400, 400, 3)\n",
      "(8, 400, 400)\n",
      "(2, 400, 400, 3)\n",
      "(2, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images_mean, groundtruths,\\\n",
    "                                                    train_size= TRAINING_SIZE/100, random_state=SEED)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T19:17:17.191957Z",
     "start_time": "2020-11-05T19:17:17.106821Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_dgIRiZFwc4",
    "outputId": "c6c286ff-fd2f-4195-ef6f-ce8878fe6a81"
   },
   "outputs": [],
   "source": [
    "\"\"\"num_epochs = NUM_EPOCHS\n",
    "\n",
    "c0 = 0  # bgrd\n",
    "c1 = 0  # road\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "\n",
    "print('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(y_train) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(y_train) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "print(len(new_indices))\n",
    "print(X_train.shape)\n",
    "X_train = X_train[new_indices, :, :, :]\n",
    "y_train = y_train[new_indices]\n",
    "\n",
    "train_size = y_train.shape[0]\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\"\"\"\n",
    "x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def train(model, criterion, dataset_train, y_train, dataset_test, y_test, optimizer, num_epochs, device):\n",
    "#   \"\"\"\n",
    "#   @param model: torch.nn.Module\n",
    "#   @param criterion: torch.nn.modules.loss._Loss\n",
    "#   @param dataset_train: torch.utils.data.DataLoader\n",
    "#   @param dataset_test: torch.utils.data.DataLoader\n",
    "#   @param optimizer: torch.optim.Optimizer\n",
    "#   @param num_epochs: int\n",
    "#   \"\"\"\n",
    "#   print(\"Starting training\")\n",
    "#   for epoch in range(num_epochs):\n",
    "#     # Train an epoch\n",
    "#     print('epoch number ',epoch)\n",
    "#     model.train()\n",
    "#     batch_x, batch_y = dataset_train, y_test\n",
    "\n",
    "#     # Evaluate the network (forward pass)\n",
    "#     prediction = model(batch_x)\n",
    "#     loss = criterion(prediction, batch_y)\n",
    "\n",
    "#     # Compute the gradient\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Update the parameters of the model with a gradient step\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Test the quality on the test set\n",
    "#     model.eval()\n",
    "#     accuracies_test = []\n",
    "\n",
    "#     batch_x, batch_y = dataset_test, y_test\n",
    "\n",
    "#     # Evaluate the network (forward pass)\n",
    "#     prediction = model(batch_x)\n",
    "#     accuracies_test.append(accuracy(prediction, batch_y))\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     print(\"Epoch {} | Test accuracy: {:.5f}\".format(epoch, sum(accuracies_test).item()/len(accuracies_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.unet_model import *\n",
    "# from scripts.train import train_net\n",
    "\n",
    "# unet = UNet(3, 2)\n",
    "# if not torch.cuda.is_available():\n",
    "#     print('Training will be ')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# unet\n",
    "# print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "# def reshape(array):\n",
    "#     return np.moveaxis(array, 3, 1)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# train(unet, criterion, torch.tensor(reshape(X_train)), torch.tensor(y_train), torch.tensor(reshape(X_test)), torch.tensor(y_test), optimizer, NUM_EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T18:57:42.968972Z",
     "start_time": "2020-11-05T18:57:42.896467Z"
    },
    "id": "cBE8XdxDFwc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set to true\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 400, 400, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 400, 400, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 400, 400, 16) 2320        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 200, 200, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 200, 200, 32) 4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 200, 200, 32) 9248        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 100, 100, 32) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 100, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 100, 64) 36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 50, 50, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 50, 50, 128)  147584      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 128)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 256)  590080      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 50, 50, 256)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 50, 50, 384)  0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 50, 50, 128)  442496      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 100, 100, 128 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 100, 192 0           up_sampling2d_1[0][0]            \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 100, 64) 110656      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 200, 200, 64) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 200, 96) 0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 200, 200, 32) 27680       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 200, 200, 32) 9248        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 400, 400, 32) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 400, 400, 48) 0           up_sampling2d_3[0][0]            \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 400, 400, 16) 6928        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 400, 400, 16) 2320        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 400, 400, 1)  17          conv2d_17[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,962,625\n",
      "Trainable params: 1,962,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv2d/Relu (defined at <ipython-input-20-bcac12c583c5>:16) ]] [Op:__inference_train_function_2429]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-bcac12c583c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             metrics=['accuracy'])\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVALIDATION_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv2d/Relu (defined at <ipython-input-20-bcac12c583c5>:16) ]] [Op:__inference_train_function_2429]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "if RESTORE_MODEL:\n",
    "    # It can be used to reconstruct the model identically.\n",
    "    model = models.load_model(model_chosen['save_dir'])\n",
    "else: \n",
    "    #This cell is required for me (Ben), otherwise I get \"convolution algorithm not found\"\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        print(\"Set to true\")\n",
    "        print(physical_devices)\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    model = model_chosen['model']()\n",
    "    model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs = NUM_EPOCHS ,validation_split=VALIDATION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:36:07.202726Z",
     "start_time": "2020-11-04T15:36:05.671002Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fon8F-kTFwc5",
    "outputId": "85782f7b-4254-4fa3-8222-af7a391abd68"
   },
   "outputs": [],
   "source": [
    "if not RESTORE_MODEL:\n",
    "    model.save(model_chosen['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:31:17.904771Z",
     "start_time": "2020-11-04T15:31:11.581527Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujs3K6_xFwc5",
    "outputId": "13b04fbb-722a-481d-c6a2-1e0d631cc97b"
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "print(pred_train)\n",
    "print(\"Training error rate: {:.2f}%\".format(error_rate(pred_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:29:40.055957Z",
     "start_time": "2020-11-04T15:29:38.082308Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6Ma8H_5Fwc5",
    "outputId": "53dd4e8b-9628-4971-ec39-ca14cb4c5761"
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "print(\"Test error rate: {:.2f}%\".format(error_rate(pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:33:21.635243Z",
     "start_time": "2020-11-04T15:33:21.355409Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "3BbciowpFwc5",
    "outputId": "2d337f9f-ad99-44e1-8df7-a92c9143e7fe"
   },
   "outputs": [],
   "source": [
    "F1_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:42:12.295880Z",
     "start_time": "2020-11-04T15:41:40.987577Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "La4cKwBaFwc5",
    "outputId": "543d6ebf-ec19-42ad-e314-34645305b9b4"
   },
   "outputs": [],
   "source": [
    "print(\"Running prediction on training set\")\n",
    "prediction_training_dir = PREDICTIONS_SAVE_DIR + \"predictions_training/\"\n",
    "if not os.path.isdir(prediction_training_dir):\n",
    "    os.mkdir(prediction_training_dir)\n",
    "for i in range(1, TRAINING_SIZE + 1):\n",
    "    pimg = get_prediction_with_groundtruth(model, train_data_filename, i)\n",
    "    Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i) + \".png\")\n",
    "    oimg = get_prediction_with_overlay(model, train_data_filename, i)\n",
    "    oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T16:09:16.623322Z",
     "start_time": "2020-11-04T16:09:02.803475Z"
    },
    "id": "YVBsfv2kFwc6",
    "outputId": "a665b18d-3c33-42e6-a4b0-1d35b48ddfa0"
   },
   "outputs": [],
   "source": [
    "if GENERATE_PREDICTION :\n",
    "    predict_test_masks(model)\n",
    "    masks_to_submission(\"submission.csv\", PREDICTIONS_SAVE_DIR+\"predictions_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "U-net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu2]",
   "language": "python",
   "name": "conda-env-tf-gpu2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
